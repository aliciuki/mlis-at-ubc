{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curating TweetJSON with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tkinter import filedialog as fd\n",
    "import codecs\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Simplify TweetJSON\n",
    "\n",
    "The <code>simplify()</code> function contains <code>remove()</code> and <code>qtd_twts()</code>. The first of the two deletes unnecessary elements from each tweet, then writes the simpler JSON into a new file with a suffix <code>_simple.jsonl</code>, and the second writes all cited tweets (tweet objects) into a new file (<code>_quoted.jsonl</code>) that can be reprocessed with <code>simplify()</code> until <code>qtd_twts()</code> returns an empty .jsonl file.\n",
    "\n",
    "#### Open input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calls a function to pop up a GUI window to select a directory and input file. \n",
    "# This function (fd.askopenfilename()) is called inside codecs.open, with an encoding argument to maintain utf-8.\n",
    "inf = fd.askopenfilename(title='Select a *.JSON or *.JSONL (Tweet JSON) file to simplify')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify(f):\n",
    "    remove(f)\n",
    "    qtd_twts(f)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove(file):\n",
    "# Prompts user to enter an output file name, which is saved in the current directory.\n",
    "    out = input(\"How should I call the simplified version of this file? ([filename]_simple.jsonl): \")\n",
    "    # Creates lists for the superfluous Tweet JSON elements in each object:\n",
    "    t_del = ['id','truncated','in_reply_to_status_id','in_reply_to_user_id','quoted_status_id','extended_entities','favorited','retweeted','possibly_sensitive','filter_level','matching_rules','current_user_retweet','scopes','withheld_copyright','withheld_in_countries','withheld_scope','geo']\n",
    "    u_del = ['id','name','derived','url','protected','profile_banner_url','profile_image_url_https','default_profile','default_profile_image','withheld_in_countries','withheld_scope','utc_offset','time_zone','lang','geo_enabled','following','follow_request_sent','has_extended_profile','notifications','profile_location','contributors_enabled','profile_image_url','profile_background_color','profile_background_image_url','profile_background_image_url_https','profile_background_tile','profile_link_color','profile_sidebar_border_color','profile_sidebar_fill_color','profile_text_color','profile_use_background_image','is_translator','is_translation_enabled','translator_type']\n",
    "    e_del = ['media','url','urls','symbols','polls','description']\n",
    "    u_e_del = ['description', 'media','url','urls']\n",
    "    with codecs.open(file, encoding='utf-8') as f:\n",
    "        #Opens the output file with a suffix (simple) and .JSON filetype, in mode \"append\". \n",
    "        #New output file will be created if it does not already exist (+), and any lines will be added to the end.\n",
    "        with io.open(out+\"_simple.jsonl\",\"a+\",encoding=\"utf-8\") as o:\n",
    "            #A variable 'counter' will keep track of how many tweets have been read and written. Function prints counter every 1000th time to give me a sense of how/whether progress is made.\n",
    "            counter = 0\n",
    "            # A for-loop iterates over all lines in input file, counting each line and <code>json.loads()</code> decodes it into a Python dictionary:\n",
    "            for line in f:\n",
    "                counter = counter + 1\n",
    "                # The text string in each line is decoded from JSON into a Python dictionary. \n",
    "                #Four nested for-loops (one for each set of deletions) remove any elements from the deletion lists.             \n",
    "                l = json.loads(line, encoding='utf-8')\n",
    "                for i in t_del:\n",
    "                    if i in l:\n",
    "                        del l[i]\n",
    "                for u in u_del:\n",
    "                    if u in l['user']:\n",
    "                        del l['user'][u]\n",
    "                for e in e_del:\n",
    "                    if e in l['entities']:\n",
    "                        del l['entities'][e]\n",
    "                for ue in u_e_del:\n",
    "                    if ue in l['user']['entities']:\n",
    "                        del l['user']['entities'][ue]\n",
    "                # Then, the modified line is reencoded in JSON (allowing for non-ascii characters). The JSON line is written into the output file, followed by a line break (otherwise tweets are printed all in one line, which is not good):\n",
    "                linea = json.dumps(l, ensure_ascii=False)\n",
    "                o.write(linea+'\\n')\n",
    "                if counter % 1000 == 0:# Every 1000 lines, the console prints how many tweets have been read and written.\n",
    "                    print(str(counter))\n",
    "    return# The function returns 'None'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qtd_twts(file):\n",
    "    out = input(\"How should I call the output file containing QUOTED TWEETS? ([filename]_quoted.jsonl) \")\n",
    "    with codecs.open(file, encoding='utf-8') as f:\n",
    "        with io.open(out+\"_quoted.jsonl\",\"a+\",encoding=\"utf-8\") as o:\n",
    "            counter = 0\n",
    "            for line in f:\n",
    "                counter = counter + 1\n",
    "                l = json.loads(line, encoding='utf-8')\n",
    "                if 'quoted_status' in l:\n",
    "                    q = l['quoted_status']\n",
    "                    t = l['id_str']\n",
    "                    linea = json.dumps(q, ensure_ascii=False)\n",
    "                    lineb = json.dumps(t, ensure_ascii=False)\n",
    "                    # Two lines are saved as one JSON object, retaining the quoting tweet's ID.\n",
    "                    o.write('{{\"id_str\":'+lineb+'},'+linea+'}\\n')\n",
    "                if counter % 1000 == 0:\n",
    "                    print(str(counter))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplify(inf)# This calls the main function that calls the two sub-functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Separate Files for Each Object Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_in = fd.askopenfilename(title='Select a *_simple.JSONL file as input')\n",
    "def objects(infile):\n",
    "    objtweets(infile)\n",
    "    objusers(infile)\n",
    "    objmentions(infile)\n",
    "    objhashtags(infile)\n",
    "    return\n",
    "def objtweets(inf):\n",
    "    out = input(\"How should we call the output file containing TWEETS ([filename]_tweets.jsonl): \")\n",
    "    with codecs.open(inf, encoding='utf-8') as f:\n",
    "        with io.open(out+\"_tweets.jsonl\",\"a+\",encoding=\"utf-8\") as o:\n",
    "            counter = 0\n",
    "            tw_del = ['user','entities','quoted_status']\n",
    "            for line in f:\n",
    "                counter = counter + 1\n",
    "                l = json.loads(line, encoding='utf-8')\n",
    "                for d in tw_del:\n",
    "                    del l[d]\n",
    "                    linea = json.dumps(l, ensure_ascii=False)\n",
    "                    o.write(linea+'\\n')\n",
    "                if counter % 1000 == 0:\n",
    "                        print(str(counter))\n",
    "    return\n",
    "def objusers(inf):\n",
    "    out = input(\"How should we call the output file containing USERS ([filename]_users.jsonl): \")\n",
    "    with codecs.open(inf, encoding='utf-8') as f:\n",
    "        with io.open(out+\"_users.jsonl\",\"a+\",encoding=\"utf-8\") as o:\n",
    "            counter = 0\n",
    "            uid = []\n",
    "            for line in f:\n",
    "                counter = counter + 1\n",
    "                l = json.loads(line, encoding='utf-8')\n",
    "                u = l['user']['id_str']\n",
    "                if u not in uid:\n",
    "                    uid.append(l['user']['id_str'])\n",
    "                    usr = l['user']\n",
    "                    linea = json.dumps(usr, ensure_ascii=False)\n",
    "                    o.write(linea+'\\n')\n",
    "                if counter % 1000 == 0:\n",
    "                        print(str(counter))\n",
    "    return\n",
    "def objmentions(file):\n",
    "    out = input(\"How should I call the MENTIONS file? ([filename]_mentions.jsonl): \")\n",
    "    with codecs.open(file, encoding='utf-8') as f:\n",
    "        with io.open(out+\"_mentions.jsonl\",\"a+\",encoding=\"utf-8\") as o:\n",
    "            counter = 0\n",
    "            for line in f:\n",
    "                counter = counter + 1\n",
    "                l = json.loads(line, encoding='utf-8')\n",
    "                m = l['entities']['user_mentions']\n",
    "                t = l['id_str']\n",
    "                linea = json.dumps(m, ensure_ascii=False)\n",
    "                lineb = json.dumps(t, ensure_ascii=False)\n",
    "                o.write('{{\"id_str\":'+lineb+'},'+linea+'}\\n')\n",
    "                if counter % 1000 == 0:\n",
    "                    print(str(counter))\n",
    "    return\n",
    "def objhashtags(file):\n",
    "    out = input(\"How should I call the HASHTAGS file? ([filename]_hashtags.jsonl) \")\n",
    "    with codecs.open(file, encoding='utf-8') as f:\n",
    "        with io.open(out+\"_hashtags.jsonl\",\"a+\",encoding=\"utf-8\") as o:\n",
    "            counter = 0\n",
    "            for line in f:\n",
    "                counter = counter + 1\n",
    "                l = json.loads(line, encoding='utf-8')\n",
    "                h = l['entities']['hashtags']\n",
    "                t = l['id_str']\n",
    "                linea = json.dumps(h, ensure_ascii=False)\n",
    "                lineb = json.dumps(t, ensure_ascii=False)\n",
    "                o.write('{{\"id_str\":'+lineb+'},'+linea+'}\\n')\n",
    "                if counter % 1000 == 0:\n",
    "                    print(str(counter))\n",
    "    return\n",
    "objects(file_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Merge and Deduplicate Files Containing the Same Object Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = fd.askopenfilename(title='Select the 1st (of 3) JSON file to concatenate')\n",
    "tweets_at = fd.askopenfilename(title='Select the 2nd (of 3) JSON file to concatenate')\n",
    "tweets_from = fd.askopenfilename(title='Select the third (of 3) JSON file to concatenate')\n",
    "\n",
    "filenames = [hashtags,tweets_at,tweets_from]\n",
    "\n",
    "def one_file(files):\n",
    "    out = input(\"How should I call the file to unite them all? ([filename]_onefile.jsonl) \")\n",
    "    with codecs.open(out+'_onefile.jsonl', 'a+', encoding='utf-8') as f:\n",
    "        for name in files:\n",
    "            with codecs.open(name,'r', encoding='utf-8') as infile:\n",
    "                for line in infile:\n",
    "                    f.write(line)\n",
    "\n",
    "one_file(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupefile = fd.askopenfilename(title='Select a JSONL file that may have duplicate lines')\n",
    "\n",
    "def dedupe(dupe):\n",
    "    out = input(\"How should I call the DEDUPED file? ([filename]_deduped.jsonl): \")\n",
    "    with codecs.open(out+'_deduped.jsonl', 'a+', encoding='utf-8') as o:\n",
    "        with codecs.open(dupe,'r', encoding='utf-8') as d:\n",
    "            seen = set()\n",
    "            for l in d:\n",
    "                if l not in seen:\n",
    "                    o.write(l)\n",
    "                    seen.add(l)\n",
    "            o.close()\n",
    "    return\n",
    "dedupe(dupefile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
